%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper
\pdfobjcompresslevel=2
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{graphicx}
\usepackage{cite}
% \documentclass{article}
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{algorithm}
\usepackage{hyperref}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{xcolor}
% \title{\LARGE \bf
% Visual Feedback in Instruction-Following Tasks for\\ LLM-based Embodied Agent
% }
\title{\LARGE \bf
LERa: Replanning with Visual Feedback in Instruction Following
}

% \author{
% Anonymous Authors
% }
\author{Svyatoslav~Pchelintsev$^{1*}$, Maxim~Patratskiy$^{1*}$, Anatoly~Onishchenko$^{1}$, Alexandr~Korchemnyi$^{1}$, \\Aleksandr~Medvedev$^{2}$, Uliana~Vinogradova$^{2}$, Ilya~Galuzinsky$^{2}$, Aleksey~Postnikov$^{2}$, \\Alexey~K.~Kovalev$^{1,3}$, and Aleksandr~I.~Panov$^{1,3}$
\thanks{*Svyatoslav~Pchelintsev and Maxim~Patratskiy contributed equally to this work. $^{1}$MIPT, Dolgoprudny, 141701, Russia $^{2}$Sberbank of Russia, Robotics Center, Moscow, 117997, Russia $^{3}$AIRI, Moscow, 121170, Russia {\tt\small \{kovalev,panov\}@airi.net}}%
}
% \thanks{*These authors contributed equally to this work.}\thanks{$^{1}$MIPT, Dolgoprudny, 141701, Russia}
% \thanks{$^{2}$Sberbank of Russia, Robotics Center, Moscow, 117997, Russia}
% \thanks{$^{2}$AIRI, Moscow, 105064, Russia {\tt\small albert.author@papercept.net}}%
% }


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Large Language Models are increasingly used in robotics for task planning, but their reliance on textual inputs limits their adaptability to real-world changes and failures. To address these challenges, we propose LERa -- \underline{L}ook, \underline{E}xplain, \underline{R}epl\underline{a}n~-- a Visual Language Model-based replanning approach that utilizes visual feedback. Unlike existing methods, LERa requires only a raw RGB image, a natural language instruction, an initial task plan, and failure detection—without additional information such as object detection or predefined conditions that may be unavailable in a given scenario. The replanning process consists of three steps: (i) Look, where LERa generates a scene description and identifies errors; (ii) Explain, where it provides corrective guidance; and (iii) Replan, where it modifies the plan accordingly. LERa is adaptable to various agent architectures and can handle errors from both dynamic scene changes and task execution failures. We evaluate LERa on the newly introduced ALFRED-ChaOS and VirtualHome-ChaOS datasets, achieving a 40\% improvement over baselines in dynamic environments. In tabletop manipulation tasks with a predefined probability of task failure within the PyBullet simulator, LERa improves success rates by up to 67\%. Further experiments, including real-world trials with a tabletop manipulator robot, confirm LERa’s effectiveness in replanning. We demonstrate that LERa is a robust and adaptable solution for error-aware task execution in robotics.
% Recently, large language models (LLMs) have been widely used for task planning in embodied instruction following. However, an LLM often fails to take into account the specifics of the particular scene in which the agent is operating. Various approaches have been proposed to address this issue. One direction of research is the correction of errors that occur during plan execution, usually by providing textual feedback, i.e., information about the state of the environment in textual form. On the other hand, visual feedback, i.e., feedback is represented by images, in LLMs for embodied artificial intelligence tasks has not been thoroughly investigated. In this paper, we propose LERa -- \underline{L}ook, \underline{E}xplain, \underline{R}epl\underline{a}n~--~an approach that uses visual feedback to correct plans when errors occur during task execution. We evaluate our approach in different environments and show that LERa achieves up to 60\% improvement in success rate compared to baseline agents. We also demonstrate the generalizability of LERa to real-world environments by conducting experiments on a table-top manipulation robot.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
\label{sec:intro}
Large Language Models (LLMs) trained on Internet-scale data can solve problems that they were not originally designed for~\cite{kojima2022large}. This has led to the widespread use of LLMs in robotics for task planning~\cite{huang2022language,kovalev2022application,sarkisyan2023evaluation,10160591,10801328,onishchenko2025lookplangraph}.

However, there is no guarantee that a robot will successfully execute a generated task plan in a real environment. This is because LLMs rely on common sense knowledge acquired during training and do not account for dynamic environmental changes or task failures. For example, given the instruction \textit{``Heat a slice of pizza,''} the model might generate a plan that includes the task \textit{``Open the microwave.''} However, if the microwave is already open -- perhaps because the previous user forgot to close it -- the plan may result in an error, preventing successful execution. Similarly, if the slice of pizza falls while being placed inside, the robot may continue executing the plan without recognizing the failure, leading to an incomplete or incorrect outcome.

To address this problem, various LLM-based replanning approaches have been proposed~\cite{song2023llm,guo2024doremi,joublin2024copal}. However, LLMs rely solely on textual input, which does not fully capture the state of the environment and requires this state to be represented in textual form, such as a list of objects or a scene description~\cite{huang2022language,10161317,10160591,10801328,song2023llm}. This textual representation is typically derived from the output of computer vision systems, such as object detectors or semantic segmenters. Visual Language Models (VLMs), on the other hand, can directly process both visual and textual modalities, making them more suitable for capturing the state of the environment and enabling replanning. However, most modern VLM-based replanning approaches: (i) are designed for a specific task planner or agent architecture, making them difficult to adapt to different architectures~\cite{zhang2023grounding,yang2024guidinglonghorizontaskmotion,mei2024replanvlm,skreta2024replan}, (ii) require not only information from the RGB camera and the instruction with the corresponding task plan for replanning but also additional data, such as pre- and post-action conditions, target object positions, and object bounding boxes -- information that may not always be available in a given scenario~\cite{yang2024guidinglonghorizontaskmotion,mei2024replanvlm}, (iii) are primarily designed to handle a single type of error, limiting their applicability to other error types~\cite{zhang2023grounding,yang2024guidinglonghorizontaskmotion,skreta2024replan}.
\begin{figure}
    \centering
 \includegraphics[width=1\columnwidth]{images/fig1_visual_abstract.pdf}
    \caption{
    An approach without error handling can be enhanced with the LERa module to dynamically update the queue of subtasks when errors occur during plan execution. On the left (solid arrows), the plan is generated once at the beginning of an episode and remains unchanged throughout. With LERa (dashed arrows), when an error occurs during the execution of the current subtask, a Visual Language Model is used to replan based on visual feedback.
    }
    \label{fig:visab}
    \vspace{-10pt}
\end{figure}

In this paper, we address these problems by proposing \textbf{LERa} -- \textbf{\underline{L}}ook, \textbf{\underline{E}}xplain, \textbf{\underline{R}}epl\textbf{\underline{a}}n -- a VLM-based approach for replanning with visual feedback. LERa uses only basic information for replanning: (i) a raw RGB camera image, without requiring detected objects, masks, or any other additional information, (ii) information about action execution failure, without needing an explanation of the reason for the failure, (iii) the initial natural language instruction, and (iv) the task plan. LERa makes no specific assumptions about the agent’s architecture and can extend any agent that detects action execution failure and requests replanning (see Fig.~\ref{fig:visab}). The replanning process in LERa consists of three steps. \textbf{Look} -- given the raw image from the RGB camera and information about the action failure, LERa generates a scene description and identifies the cause of the error. \textbf{Explain} -- based on the identified information, LERa explains how to correct the plan and addresses the replanning problem without being constrained to a specific output format. \textbf{Replan} -- LERa modifies the old plan by incorporating the proposed changes and passes the final version of the plan to the agent for execution. LERa is capable of handling errors from dynamic environmental changes and action execution failures.

To evaluate LERa's ability to replan when errors occur due to dynamic environmental changes, we collected the \textbf{ALFRED-ChaOS} and \textbf{VirtualHome-ChaOS} datasets, each containing more than 500 episodes, based on the well-known ALFRED~\cite{shridhar2020alfred} and VirtualHome~\cite{puig2018virtualhome} environments. In these datasets, objects remain in their initially assumed states in half of the episodes, while their states are changed in the other half. LERa demonstrates more than a 40 percent improvement in success rate over baselines, including a VLM-based baseline that follows the common practice of one-step replanning. To assess LERa's ability to replan when task execution fails due to action failures, we conducted experiments on 10 types of tabletop manipulation tasks with a predefined action failure probability in the PyBullet simulator~\cite{coumans2021}. LERa shows up to a 67 percent improvement in success rate over a baseline that lacks replanning capabilities and achieves better, yet comparable, results compared to a baseline that includes replanning. This outcome can be attributed to the simplicity of visual observations and tasks in the environment itself.

We also conducted extensive ablation experiments, confirming the necessity of all three steps in the replanning process. Additionally, we performed experiments with different VLMs, demonstrating how VLM quality affects replanning performance. Further experiments with a non-ideal subtask checker on ALFRED-ChaOS show the consistent superiority of LERa when working with imperfect error detection. To demonstrate the generalizability of our approach to real-world scenarios, we integrated LERa into the control system of a real robot for tabletop manipulation. LERa successfully handled replanning in 15 out of 18 trials.

Our main contributions can be summarized as follows:
\begin{enumerate}
    \item \textbf{We propose LERa}, a VLM-based replanning approach that utilizes visual feedback, makes minimal assumptions about the agent's architecture, and is capable of replanning for both dynamic scene changes and action execution errors.
    \item \textbf{We introduce three different environments}, based on well-known simulators, that can be used to evaluate replanning approaches.
    \item \textbf{We conduct rigorous experiments and ablations across various environments}, demonstrating that LERa successfully handles replanning under different conditions and for different types of errors.
    \item \textbf{We validate the applicability of LERa to real-world problems} by integrating it into a real robot control system for tabletop manipulation, showing that LERa effectively handles replanning.
\end{enumerate}

\section{RELATED WORK}
VLM-based approaches are widely used in robotics for replanning, but they usually either rely on additional information beyond the camera image, assume a specialized agent architecture, or replan only for a specific type of error. In~\cite{hu2023look}, the VLM is used as the initial task planner, and the replanning problem is not structurally separated. Although this approach significantly simplifies the architecture, it does not account for the specific features of the environment or the robot in the initial planning~\cite{yang2024guidinglonghorizontaskmotion}.
In~\cite{skreta2024replan}, a complex interaction structure between planning modules at different levels is used, where VLM is first employed to confirm the correctness of an action choice before its execution in the environment. In~\cite{zhang2023grounding}, replanning is framed as a visual question-answering problem, handling only errors related to action failures and requiring information about the preconditions and effects of actions.
In~\cite{mei2024replanvlm}, a VLM-based Extra Bot is used as an action failure detector by analyzing frames before and after an action is performed. Replanning is handled by the Design Bot, which also functions as a task planner, imposing constraints on the agent's architecture.
In~\cite{yang2024guidinglonghorizontaskmotion}, VLM is used to generate intermediate goals instead of actions during replanning. In this case, information about detected objects, their relationships, and collisions is provided to VLM along with the image. In~\cite{duan2024aha}, VLM is further trained to detect and explain errors.

Our approach differs from all these methods in that LERa uses only the minimum information necessary for replanning and does not make specific assumptions about the planner architecture. This improves its generalizability and simplifies its use. It can handle multiple types of errors and does not require further training of the VLM.

\section{METHOD}
\label{sec:method}
We propose LERa, a VLM-based approach for replanning (Section~\ref{subsec:problem}) with visual feedback. LERa makes no specific assumptions about the agent's architecture and can be used with any agent that provides action failure detection and replanning request capabilities (Section~\ref{subsec:lera}). It employs a three-step prompting strategy (Section~\ref{subsec:prompting}) that allows it to handle different types of errors without requiring additional information. LERa relies solely on an RGB image, action failure detection information, an initial natural language instruction, and a task plan.

\subsection{Problem Formulation}
\label{subsec:problem}
We consider the problem of \textit{replanning} as the problem of adjusting the initial plan $P$, which consists of a sequence of actions (tasks) $P=(a_1, a_2, \dots, a_n)$ (where $n$ is a plan length) executable in the environment, upon receiving information about the occurrence of an error during the execution of an action (task). We do not consider low-level actions -- those resulting from motion planning rather than task planning -- as components of the plan. Therefore, we use the terms \textit{``action''} and \textit{``task'}' synonymously where this does not cause confusion.
By \textit{plan adjustment}, we mean modifying the sequence of actions in the plan $P=(a_1, a_2, \dots, a_n)$ to $P'=(a'_1, a'_2, \dots, a'_{n'})$ so that execution can successfully continue in the environment. Since we do not address the problem of initial planning, we assume that the plan $P$ always leads to achieving the goal in the absence of environmental changes or unsuccessful action execution.

We assume that the replanner $R$ operates only on information available after the planning stage -- namely, natural language instructions $I$, the task plan $P$, and information about the occurrence of an error at time $t$ (denoted as $E_t$), obtained using the Subtask Checker $SC$. Additionally, $R$ has access to the current observation $O_t$ from an RGB camera but does not use any other information.
Thus, the problem is reduced to creating a function $R$ that, given all the described inputs, generates a corrected plan $P'$: $R(O_t, E_t, I, P) = P'$.

\subsection{The LERa Approach}
\label{subsec:lera}
LERa makes fairly general assumptions about an agent's architecture, requiring three main modules: (1)~Task Planner, (2)~Subtask Executor, and (3)Subtask Checker (Fig.\ref{fig:method_overview}).
The Task Planner (1) generates a task plan $P$ from language instructions $I$ and additional information, managing task execution by assigning the next action $a_i$.  The Subtask Executor (2) produces low-level actions, potentially incorporating navigation and motion planning. The The Subtask Checker (3) verifies task execution using environment feedback, requesting the next action $a_{i+1}$ if successful or triggering replanning from LERa (4) if an error occurs.

The LERa module updates the agent’s plan $P$ to a revised plan $P'$, mitigating execution errors. It either sends $P'$ to the Task Planner or directly provides a corrected action $a'_{i+1}$ to the Subtask Executor. LERa operates in three main steps: \textit{Look}, \textit{Explain}, and \textit{Replan}. The algorithm for LERa's operation is presented in Algorithm~\ref{alg:alg}, and Fig.~\ref{fig:replan} illustrates a step-by-step example of replanning.

\begin{figure}[t]
    \centering
\includegraphics[width=\linewidth]{images/fig2_example_cr.pdf}
\vspace{-10pt}
    \caption{A step-by-step example of replanning using the LERa module. It consists of three main steps: Look, Explain, and Replan. Each step solves its own problem in order to modify the current plan $P$ to a new plan $P'$.}
    \label{fig:replan}
    \vspace{-15pt}
\end{figure}

\textbf{Look Step:} At the moment of task $a_t$ execution failure ($SC(a_t) = 0$), the agent captures the visual observation $O_t$. Based on $O_t$ and $a_t$, LERa creates a scene description and provides examples of various failure reasons. The predicted description is called $L$.  

\textbf{Explain Step:} Given the current plan $P$, the failed action $a_t$, and the information from the previous step $L$, LERa outputs a textual description of the error and a conceptual sequence of new actions -- $E$.  

\textbf{Replan Step:} At this step, LERa constructs a new action plan $P' = (a_t', a_{t+1}', \dots, a_n')$. The new plan is generated without any additional explanation of its applicability or validation of execution feasibility.  

The main experiments were performed with Oracle versions of the agent modules, which are capable of executing high-level actions with minimal errors, as our goal was to isolate LERa's replanning performance from errors in other modules.
\begin{algorithm}[h]
\caption{LERa Replanning Process}
\begin{algorithmic}[1]
\Require \\
    Natural language instruction $I$, \\
    Current observation $O_t$, \\
    Current task plan $P = (a_t, ...,a_n)$, \\
    Failed action $a_t$
\Ensure Updated action plan $P'$ that ensures successful task completion
\State $L \leftarrow \text{Look}(o_t, a_t)$ \Comment{Generate scene description and identify failure cause}
\State $E \leftarrow \text{Explain}(I,  L, P)$ \Comment{Explain error and propose conceptual plan}
\State $P' \leftarrow \text{Replan}(I,  E, P)$ \Comment{Predict updated sequence of actions}
\State \textbf{return} $P'$
\end{algorithmic}
\label{alg:alg}
\end{algorithm}
\vspace{-10pt}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{images/fig3_agent.pdf}
  \caption{The proposed structure of an agent to use LERa consists of three modules: (1) Task Planner, (2) Subtask Executor, and (3) Subtask Checker. \textcolor{blue}{Blue} is for language, \textcolor{yellow}{yellow} is for planning and execution, \textcolor{red}{red} is for self-checking, and \textcolor{green}{green} is for replanning.}
  \label{fig:method_overview}
  \vspace{-10pt}
  % \hfill
\end{figure*}

\subsection{Prompting Strategies}
\label{subsec:prompting}
The LERa method utilizes three different prompt templates for each step to solve specific tasks (Fig.~\ref{fig:prompt_templates}). In the \textbf{Look Step}, LERa defines the reasons for the error and provides simple ideas for a solution. To prevent the VLM from being overloaded with unnecessary information, only a limited set of key facts about the virtual environment is included in the system instruction. For the same reason, only the first step of the current plan is provided to the model along with the observation. The \textbf{Explain Step} is designed to address the replanning task without being constrained by any output format. The objective of this stage is to reason about the given task instruction, the existing plan, and the newly acquired visual information in order to generate an updated task plan. In addition, the model is required to provide an explanation for each predicted step, ensuring that the output contains as much relevant information as possible to support the final replanning step. The \textbf{Replan Step} constructs the final corrected task plan. Its prompt includes few-shot examples to constrain formatting, a list of available environment actions with descriptions, the existing task plan, and the newly predicted task plan with step explanations. This step ensures the new plan is executable in the virtual environment, as the Explain Step may produce infeasible actions. Additionally, the Replan Step refines the plan to align with the environment or correct logical errors.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\columnwidth]{images/fig4_prompt.pdf} % Контроль высоты
    \caption{Prompt templates used by the LERa module.}
    \label{fig:prompt_templates}
\end{figure}

\section{Experiments}
\label{sec:experiments}
% \subsection{Environments}
\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{images/fig5_all_env_one_row.png}
    \caption{We evaluate LERa across diverse set of environments. From left to right, visual observations from: ALFRED, VirtualHome, TableTop-PyBullet, and TableTop-RoboticStand.
    \vspace{-30pt}
    }
    \label{fig:all-envs}
\end{figure}
To demonstrate the generalizability of the LERa approach, we evaluated it in diverse simulation environments and on a real table-top manipulation robot (Fig.~\ref{fig:all-envs}). We conducted experiments with different parts of LERa to assess the contribution of each component to overall success. Additional evaluations focused on measuring the impact of the VLM on the overall success rate. Moreover, we tested LERa with a non-ideal checker to evaluate its ability to correctly assess the environment state and restore the original plan when no actual errors occurred. In addition, we conducted experiments on a real robot to demonstrate the applicability of the method in real world conditions.

\subsection{Environments}
We conducted diverse set of experiments in three virtual environments: ALFRED~\cite{shridhar2020alfred}, VirtualHome~\cite{puig2018virtualhome} and PyBullet. In ALFRED and VirtualHome, experiments focused on assessing the approach’s ability to handle errors caused by unexpected object state changes. In PyBullet, we evaluated its effectiveness in scenarios where the last executed action completed, but the intended goal was not achieved. Such tasks require the agent to detect partial failure, infer its cause, and adjust the plan accordingly

\textbf{ALFRED-ChaOS.}
In ALFRED, an embodied agent is asked to complete a household task, e.g. ``Put a clean sponge on a metal rack'', by giving low-level instructions, e.g. ``Go to the left and face the faucet side of the bathtub. Pick up left most \dots''. We took episodes from ALFRED's ``valid\_seen'' and ``valid\_unseen'' splits, filtered out the repetitive ones as they corresponded to the same ground truth (GT) plans, and changed states of objects \textbf{that affect the execution of the task}. As a result, we obtained 251 episodes with 290 cases of objects with changed states. We combined these episodes with their original versions and ended up with a total of 502 episodes. We refer to the resulting dataset as \textbf{ALFRED-ChaOS}, which stands for \textbf{Cha}nged \textbf{O}bject \textbf{S}tates.

\textbf{VirtualHome-ChaOS.}
VirtualHome is similar to ALFRED as an embodied agent is asked to complete household tasks.
In this environment, we adapted three types of tasks: heating objects in the microwave, placing objects in the refrigerator, and washing objects in the dishwasher. For each of the 50 scenes, we collected two distinct tasks for each task type, involving different objects. Scenes that lacked essential containers for the tasks were excluded from the dataset.
Using this approach, we created 262 tasks requiring interactions with various objects in the scene. To implement changing in objects states, we modified the states of key containers relevant to each task. Combining these modified tasks with the original ones resulted in a total of 524 tasks. We refer to the resulting dataset as \textbf{VirtualHome-ChaOS}.

\textbf{TableTop-PyBullet.}
In Alfred and Virtual Home we evaluated replanning capabilities in scenarios where the object's state differed from the initial state. To evaluate the ability to resolve errors that occurred during action execution, we formed a series of experiments in PyBullet. Agent has two primary actions: `pick' and `place' but may drop the object during their execution with predefined probability. We have prepared 10 tasks and run each 10 times with different object's locations and drop moments resulting in 100 scenarios. Each task has a different plan length and difficulty. For example ``Place red block in red bowl'' and ``Build two towers in bowls: blue on red, yellow on green'' have lengths of one and four pick-place action pairs. In the first task agent must perform action sequence of: ``locate red block'', ``pick red block'', ``locate red bowl'', ``place red bowl''. The second task plan is four times larger, as agent has to move four blocks instead of one. 

\textbf{TableTop-RoboticStand.}
To evaluate LERa in a real-world environment, we use the XArm6 robotic platform, which is equipped with XArm grippers and RealSense L515 and D435 sensors. It performs manipulation tasks with two primary actions: pick and place. The planner, based on an open-source LLM, generates the plan using object recognition data, while the fine-tuned VLM-based checker evaluates the success of the action execution. Re-planning is triggered when checker detects execution errors, such as misplacement or misclassification of objects. 
The experiments consisted of 18 different tasks involving the manipulation of various sets of objects from a table to a container. Notably, in this real world setting, errors frequently occurred due to inaccuracies in the gripper position predicted by the agent. This lead to failures in grasping or object placement. Additionally, some episodes involved initial misclassification by the visual detector, resulting in an incorrect initial plan and, consequently, the inability to correctly execute the human instruction. Due to frequent errors, replanning was triggered, allowing LERa to adjust the original plan not only to correct the last observed mistake but also to ensure the successful execution of the intended human instruction.

\subsection{Metrics}
We use the following metrics to evaluate the replanning quality: \textbf{Success Rate} (\textbf{SR}), \textbf{Goal-Condition Success} (\textbf{GCR}), and \textbf{Success Rate of Replanning} (\textbf{SRep}).  \textbf{SR} is the ratio of the number of successfully completed episodes to the total number of episodes. An episode is considered successfully completed when all target objects are in their target positions and states at the end of the episode. For ALFRED and VirtualHome  this mean all objects are in correct places and in specific state, e.g. apple is cutted and fridge is closed. For PyBullet target states are correct position of blocks in bowls, e.g. blue block on red one and both in yellow bowl. \textbf{GCS} is the ratio of the number of goal conditions successfully met at the end of an episode to all goal conditions in that episode, averaged over the number of episodes. This metric can be interpreted as fraction of all objects that met their target state. \textbf{SRep} is the ratio of the number of successful replannings module execution to the number of all replannings in an episode, averaged over the number of episodes. A replanning is considered successful if it results in the agent continuing to execute the plan resolving issue.


\subsection{Agents}
The LERa approach functions solely as a replanning module, meaning it cannot be used for initial plan generation with guarantees on its exutability. To evaluate the replanning capability of LERa, we incorporate ground-truth plans with agents that achieve the highest success rates in their respective environments.

In our experiments across all environments, we employed an Oracle agent as the main baseline. This agent is provided with a complete initial plan that solves the task assuming that there are no objects with altered states or execution errors. However, the agent not able to replan and strictly follows the predefined plan. If an action in the plan cannot be executed, the agent simply moves on to the next action.
In the VirtualHome and PyBullet environments, the agent achieves a 100\% success rate (SR) as long as there are no changes in the scene. In the ALFRED environment, when using ground-truth (GT) plans, the agent successfully completes tasks with an SR of approximately 75\%. Although we use the GT plans, ALFRED’s predefined templates are suboptimal, as they do not account for hidden objects in receptacles. For example, if an agent is retrieving a knife from one of multiple drawers, the oracle may teleport to an empty one. These imperfections reduce SR by approximately 15–20\%. An additional $\approx 5\%$ drop results from AI2THOR bugs, such as unexplained \textit{``PutObject''} failures or objects being rendered without corresponding masks.

\subsection{Baselines}
To assess the contribution of each step in LERa, we conducted experiments with its different variations -- \textbf{Ra}, \textbf{LRa}, and \textbf{ERa} -- each representing a distinct combination of replanning steps. These experiments allow for a detailed analysis of the role of each component and provides a structured comparison against the full method. Each LERa variation is an exact replica of the corresponding part of the main approach, ensuring consistency in evaluation. The ERa variant, in particular, represents one of the most commonly used error correction strategies. It does not process visual information but instead relies on textual feedback from the environment, which explicitly indicates whether a given action was unsuccessful. This makes ER a widely applicable baseline, as it simulates a scenario which the agent corrects errors based solely on system-provided failure signals rather than direct perception. The \textbf{Baseline} represents a commonly used method in recent research~\cite{zhang2023grounding,mei2024replanvlm} and serves as a mid-level standard within the field. It performs immediate replanning upon receiving visual information and additional task-related data, directly outputting a corrected plan. In our implementation, it corresponds to the ``Replan'' step of LERa, with the addition of visual feedback. However, unlike LRa, the baseline operates in a single-step manner, generating a new without intermediate reasoning.

\subsection{Ablations}
In the additional experiments, we demonstrate how the LERa module performs when other modules make mistakes. In real world scenarious it is almoust impossible to create checker detects errors with absolute accuracy. Thus The first three agents (``O-05'', ``O-10'' and ``O-15'') have imperfect Subtask Checker that flips the prediction with probabilities of 0.05, 0.10 and 0.15, respectively. The forth agent (``O-FC'') uses FILM's \cite{min2021film} Subtask Checker that has check error probability of $\approx 2.5\%$. 


\subsection{Implementation Details}
The experiments were run on two NVIDIA RTX Titan 26 Gb. One GPU was running a VLM and the other~--~a virtual environment and an ALFRED agent.  MiniGPT-v2~\cite{chen2023minigptv2}, was used for experiment with imperfect checkers. We chose this model due to its high image understanding abilities and small size. The generation hyperparameters were set so that the responses were deterministic. Virtual Home and PyBullet does not require much computational resources and was running on local PC. For experiments with proprietary models, their public API was used.
For experiments we use gpt-4o and gpt4o-mini~\cite{hurst2024gpt}, Gemini-Flash-1.5 and Gemini-Pro-1.5~\cite{team2023gemini}, LLaMA-3.2~\cite{dubey2024llama}.

\section{Results}

\begin{table*}[t] % Use table* environment
\caption{
Replanning results for different agents and environments.}
\centering
\small
\resizebox{0.9\textwidth}{!}{%
\begin{tabular}{lccc|ccc|ccc|ccc|ccc}
    \toprule
    & \multicolumn{3}{c}{\textbf{ALFRED-ChaOS (Seen)}} 
    & \multicolumn{3}{c}{\textbf{ALFRED-ChaOS (Unseen)}} 
    & \multicolumn{3}{c}{\textbf{VirtualHome-ChaOS}} 
    & \multicolumn{3}{c}{\textbf{PyBullet(gpt4o)}} 
    & \multicolumn{3}{c}{\textbf{PyBullet(Gemini)}}\\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13} \cmidrule(lr){14-16}
    \textbf{Agent} 
    & \textbf{SR$\uparrow$} & \textbf{GCR$\uparrow$} & \textbf{SRep$\uparrow$} 
    & \textbf{SR$\uparrow$} & \textbf{GCR$\uparrow$} & \textbf{SRep$\uparrow$} 
    & \textbf{SR$\uparrow$} & \textbf{GCR$\uparrow$} & \textbf{SRep$\uparrow$}
    & \textbf{SR$\uparrow$} & \textbf{GCR$\uparrow$} & \textbf{SRep$\uparrow$}
    & \textbf{SR$\uparrow$} & \textbf{GCR$\uparrow$} & \textbf{SRep$\uparrow$} \\
    \midrule
    Oracle
    & 33.04 & 50.04 & -
    & 31.65 & 51.71 & -
    & 50.00 & 85.75 & -
    & 19.00 & 32.50 & - 
    & 19.00 & 32.50 & - \\
    O-Ra 
    & 34.38 & 51.19 & 7.69
    & 34.17 & 54.08 & 14.16
    & 50.00 & 84.33 & 0.00
    & 53.00 & 61.58 & 39.13
    & 56.00 & 73.67 & 46.39 \\
    O-ERa 
    & 40.18 & 56.40 & 37.08
    & 42.81 & 61.81 & 33.33
    & 50.00 & 85.92 & 0.00
    & 75.00 & 80.50 & 71.24 
    & 72.00 & 78.25 & 65.65 \\
    O-LRa 
    & 34.38 & 51.00 & 6.73
    & 33.45 & 53.57 & 11.01
    & 93.00 & 97.04 & 87.00
    & \textbf{79.00} & \textbf{83.33} & \textbf{73.71}
    & \textbf{87.00} & \textbf{92.92} & \textbf{85.81} \\
    Baseline
    & 33.04 & 50.15 & 3.15
    & 32.01 & 51.89 & 0.98
    & 52.00 & 85.91 & 4.10
    & 67.00 & 74.92 & 59.06 
    & 82.00 & 89.08 & 78.90 \\
    O-LERa
    & \textbf{49.55} & \textbf{64.55} & \textbf{73.39}
    & \textbf{53.60} & \textbf{70.23} & \textbf{74.57} 
    & \textbf{94.06} & \textbf{98.17} & \textbf{95.03} 
    & 67.00 & 72.67 & 61.29 
    & 86.00 & 89.17 & 84.83 \\
    \bottomrule
\end{tabular}
}
\label{tab:lera_performance}
\end{table*}



% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
% \begin{table*}[]
% \centering
% \caption{}
% \label{tab:my-table}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{l|cccccc|cccc}
%                    & \multicolumn{6}{c|}{google robot tasks}                                   & \multicolumn{4}{c}{windowx tasks} \\ \cline{2-11} 
% Experiment & pick can & pick h cab & move\_near & open\_drawer & close\_drawer & place closed & spoon\_on\_towel & carrot\_on\_plate & stack\_cube & put eggpl basket \\ \hline
% openvla            & 0.0           & 0.0           & 0.0           & 0.0 & 0.0           & 0.0 & 0.0    & 0.0    & 0.0    & 0.0    \\
% ecot               & 0.0           & 0.0           & 0.0           & 0.0 & 0.0           & 0.0 & 0.0    & 0.0    & 0.0    & 0.0    \\
% cogact all steps   & 0.0           & 0.0           & 0.0           & 0.0 & 0.0           & 0.0 & 0.0    & 0.0    & 0.0    & 0.0    \\
% cogact one step    & 0.0           & 0.0           & \textbf{28.6} & 0.0 & \textbf{14.3} & 0.0 & 0.0    & 0.0    & 0.0    & 0.0    \\
% octo               & \textbf{14.3} & \textbf{14.3} & 0.0           & 0.0 & 0.0           & 0.0 & 0.0    & 0.0    & 0.0    & 0.0    \\
% rt1x               & 0.0           & 0.0           & \textbf{14.3} & 0.0 & \textbf{14.3} & 0.0 & 0.0    & 0.0    & 0.0    & 0.0    \\
% rt1x D1B           & 0.0           & 0.0           & \textbf{20.0} & 0.0 & \textbf{20.0} & 0.0 & 0.0    & 0.0    & 0.0    & 0.0    \\
% rt1x D1B v2        & 0.0           & \textbf{14.3} & \textbf{14.3} & 0.0 & \textbf{14.3} & 0.0 & 0.0    & 0.0    & 0.0    & 0.0    \\
% rt1x D1B vision    & \textbf{14.3} & 0.0           & \textbf{28.6} & 0.0 & \textbf{42.9} & 0.0 & 0.0    & 0.0    & 0.0    & 0.0    \\
% rt1x D1B vision v2 & 0.0           & 0.0           & \textbf{14.3} & 0.0 & \textbf{14.3} & 0.0 & 0.0    & 0.0    & 0.0    & 0.0   
% \end{tabular}%
% }
% \end{table*}



\begin{table*}[th!] % Use table* environment
\caption{
Performance of LERa with different VLMs across different environments.
}
\centering
\small
\resizebox{0.9\textwidth}{!}{%
\begin{tabular}{lccc|ccc|ccc|ccc}
    \toprule
    & \multicolumn{3}{c}{\textbf{ALFRED-ChaOS (Seen)}} 
    & \multicolumn{3}{c}{\textbf{ALFRED-ChaOS (Unseen)}} 
    & \multicolumn{3}{c}{\textbf{VirtualHome-ChaOS}} 
    & \multicolumn{3}{c}{\textbf{PyBullet}} \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}
    \textbf{VLM} 
    & \textbf{SR$\uparrow$} & \textbf{GCR$\uparrow$} & \textbf{SRep$\uparrow$} 
    & \textbf{SR$\uparrow$} & \textbf{GCR$\uparrow$} & \textbf{SRep$\uparrow$} 
    & \textbf{SR$\uparrow$} & \textbf{GCR$\uparrow$} & \textbf{SRep$\uparrow$} 
    & \textbf{SR$\uparrow$} & \textbf{GCR$\uparrow$} & \textbf{SRep$\uparrow$} \\
    \midrule
    LLaMA-3.2-11b
    & 35.71 & 52.01 & 11.11
    & 33.81 & 53.81 & 9.38
    & 52.00 & 74.17 & 20.00 
    & 34.00 & 45.92 & 20.54 \\
    LLaMA-3.2-90b
    & 38.84 & 54.80 & 30.58
    & 36.33 & 55.97 & 25.93
    & 54.00 & 84.25 & 8.00 
    & 64.00 & 71.08 & 57.68 \\
    Gemini-Flash-1.5
    & 46.43 & 61.61 & 67.22
    & 51.44 & 68.38 & 67.71
    & 59.40 & 72.51 & 24.00 
    & 55.00 & 66.33 & 46.75 \\
    Gemini-Pro-1.5
    & 42.19 & 56.51 & 56.16
    & 46.40 & 63.85 & 55.64
    & 65.35 & 87.87 & 41.58 
    & \textbf{86.00} & \textbf{89.17} & \textbf{84.83} \\
    gpt-4o-mini
    & 43.75 & 59.71 & 46.81
    & 46.04 & 63.97 & 49.12
    & 74.25 & 82.50 & 56.25 
    & 48.00 & 62.92 & 37.75 \\
    gpt-4o
    & \textbf{49.55} & \textbf{64.55} & \textbf{73.39}
    & \textbf{53.60} & \textbf{70.23} & \textbf{74.57}
    & \textbf{94.06} & \textbf{98.17} & \textbf{95.03} 
    & 67.00 & 72.67 & 61.29 \\
    \bottomrule
\end{tabular}
}
\label{tab:vlm_impact}
% \vspace{-10pt}
\end{table*}


\subsection{LERa Replanning Abilities}

Our experiments demonstrate that the LERa approach significantly improves task solving success rates across all scenarios. 
Table~\ref{tab:lera_performance} presents results for the Oracle, Baseline, LERa and its variations. 
In VirtualHome-ChaOS LERa successfully solves almost all tasks with 94\% SR. ALFRED-ChaOS, being more complex, in terms of plan structure, environment than VirtualHome, have a more moderate improvement; however, with LERa Oracle agent solves 50\% more tasks. In PyBullet, where single action may fail even after replanning due to stochasticity the impact of replanning is the most significant. 
% All experiments in this part were conducted with gpt4-o to unify results. 

Experiments results with LERa variations -- Ra, ERa, and LRa -- demonstrate that most tasks can be solved without the ``Explain'' step in PyBullet environments. The main reason is simplicity of PyBullet action. A detailed analysis is provided in Section~\ref{results:visual-feedback}. We were surprised to find that LR successfully solves tasks in PyBullet. To verify the consistency of these results, we conducted additional experiments using the Gemini model.
The results from the PyBullet-Gemini experiments consistently indicates that replanning has a significant impact on task success rates. LERa outperforms both Oracle and the baseline agents. This leads to conclusion that leveraging most suitable state-of-the-art language models enhances our approach effectiveness across diverse domains.


\subsection{Visual Feedback Impact}
\label{results:visual-feedback}
Experiments in ALFRED, as our first virtual environment, confirmed that visual information is critical for error understanding and further correction. However, without structured output formatting and explicit failure reasoning, tasks remain unsolvable. The O-ERa agent fails to handle simple tasks due to an incomplete understanding of the environment, while O-LRa generates clear failure explanations but lacks stable and reasonable planning. Combining these approaches led to the LERa module and the O-LERa agent, achieving the best results. The three-step replanning process, which included extracting error-relevant information, reasoning about the failure, and performing format constrained replanning, proved effective. Later experiments in VirtualHome confirmed these findings. PyBullet is more straightforward environment with less available actions and short object list. The O-LRa achieved the best results, finally predicting stable and well formated action plans. Despite this, the three-step approach remains preferable due to its stability, interpretability, and consistency.

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\linewidth]{images/fig6_robot.pdf}
    \caption{
    Hard replanning scenario in TableTop-RoboticStand experiments. Steps 2 and 4 mark error occurrences. In step 3, LERa intentionally repeats the previous action, attempting a successful retry. In step 5, after the pear slightly moved, the module successfully identified it as not being an apple. Consequently, LERa excluded the ``pick apple'' action from the plan, as no apple was present on the table, and proceeded to execute the subsequent steps.
    % Example of a hard replanning scenario in the TableTop-RoboticStand experiments. Steps 2 and 4 -- moments of error occurrence. In step 3, LERa predicts a repetition of the previous action. This behavior is intentional, as the module attempts to retry the action with the possibility of successful execution upon repetition. In step 5, after the pear slightly moved, the module successfully identified it as not being an apple. Consequently, LERa excluded the ``pick apple'' action from the plan, as no apple was present on the table, and proceeded to execute the subsequent steps.
    }
    \label{fig:robot_run}
    \vspace{-10pt}
\end{figure*}

\subsection{Impact of Different VLMs}

The results of experiments with different VLMs are presented in Table~\ref{tab:vlm_impact}. 
Although LERa does not require training and operates using off-the-shelf VLMs, its performance is highly dependent on the accuracy of the VLM's image analysis. 
A consistent decline in performance is observed across all environments as the quality of the VLM picture analysis decreases.
Additionally, the choice of VLM affects performance depending on the complexity of the environment. For simpler environments, such as PyBullet, Gemini models demonstrate superior results. In contrast, for more complex environments requiring advanced reasoning, GPT-based models perform better.

\subsection{Impact of Non-Perfect Checker}
In real-world applications, designing a flawless action execution checker is challenging. In our series of additional experiments, we demonstrate how the LERa module performs when other modules make mistakes. In Table~\ref{tab:not_ideal_checkers} it is easy to see that the use of the module significantly improves the success rate in all cases. Despite the high error rate of Subtask Checker, LERa is able to cope with various situations that were not described in few-shot prompts.

\begin{table}
\caption{The impact of the imperfect checker on replanning.}
\centering
\small
% \resizebox{1\linewidth}{!}{
\begin{tabular}{lcccccc}
\toprule
& \multicolumn{3}{c}{\textbf{Seen split}} & \multicolumn{3}{c}{\textbf{Unseen split}} \\ \toprule
\textbf{Agent} & \textbf{SR$\uparrow$} & \textbf{GSR$\uparrow$} & \textbf{SRep$\uparrow$} & \textbf{SR$\uparrow$} & \textbf{GSR$\uparrow$} & \textbf{SRep$\uparrow$} \\
\midrule
O-15 & 12.50 & 35.90 & - & 12.95 & 39.75 & - \\
O-10 & 17.41 & 40.89 & - & 17.63 & 43.38 & - \\
O-05 & 24.55 & 45.01 & - & 24.82 & 47.54 & - \\
O-FC   & 33.04  & 50.04 & - & 31.65 & 51.71 & - \\ \midrule
O-L-15   & 22.32 & 44.42 & 35.48 & 20.50 & 45.29 & 25.00 \\
O-L-10   & 25.45 & 46.91 & 47.32 & 25.18 & 48.95 & 28.70 \\
O-L-05   & 34.38 & 52.57 & \textbf{54.16} & 35.97 & 55.76 & 35.43 \\
O-L-FC     & \textbf{44.64} & \textbf{59.00} & 52.83 & \textbf{46.76} & \textbf{63.46} & \textbf{49.34} \\ \bottomrule
\end{tabular}
% }
\label{tab:not_ideal_checkers}
\vspace{-20pt}
\end{table}


\subsection{Results on the Real Robot}
To evaluate the performance of the LERa module, we conducted a series of experiments on a robotic arm tasked with manipulating various objects. The purpose of these experiments was to test the capabilities of the replanning in scenarios with unexpected  errors. A total of 18 series were conducted, each of which included different configurations of objects and targets. The agent successfully completed the renovation in 15 of the 18 series, demonstrating its reliability and adaptability in dynamic conditions.
Fig.~\ref{fig:robot_run} illustrates an example of a plan execution. The initial plan specified the task: ``remove the apple and the lemon from the table''. The challenge in this scenario arises from a misclassification made by the detection module, which identified a green, round object~--~a pear~--~as an apple, incorporating it into the initial plan. Since the manipulator was unable to lift the pear, execution errors occurred. After the pear slightly rotated, the LERa module re-evaluated the object and correctly determined it was not an apple, subsequently omitting the steps in the plan related to the misclassified object. 

\subsection{Failure Analysis}
In ALFRED, one of the most common and significant challenges in replanning is the inability to infer the cause of failure from observations. Unlike in VirtualHome, where the agent’s camera is positioned slightly behind, providing a clear view of objects in most cases, ALFRED employs a first-person perspective. This often leads to ambiguity in perception, making it difficult for the replanner to determine whether the observed scene includes an open refrigerator, a microwave, or simply an empty shelf. The VirtualHome failure case arose from similar problems, with the VLM misinterpreting the opened microwave door as a closed transparent one, likely due to its low position partially excluding the door from the frame. In PyBullet, most errors arise when the VLM fails to predict the necessary ``locate object'' action before attempting to pick or place it, or when it does not fully resolve the initial instruction. The model may generate a new plan that corrects an observed dropping error but fails to account for objects already placed in their correct positions. As a result, the agent incorrectly relocates blocks that were previously positioned correctly. This behavior was particularly noticeable in the outputs of smaller models, whereas GPT and Gemini demonstrated a better understanding of why already placed blocks should not be moved.
Additionally, despite all random parameters being fixed, some intrinsic environment stochasticity was observed. In certain cases, blocks that were randomly dropped fell into positions where the gripper was unable to pick them up, leading to further task failures.
% In ALFRED, one of the most common and significant challenges in replanning is the inability to infer the cause of failure from observations. Unlike in VirtualHome, where the agent’s camera is positioned slightly behind, providing a clear view of objects in most cases, ALFRED employs a first-person perspective. This often leads to ambiguity in perception, making it difficult for the replanner to determine whether the observed scene includes an open refrigerator, a microwave, or simply an empty shelf. The VirtualHome failure case arose from similar problems, the VLM misinterpreting the opened microwave door as closed transparent, likely due to its low position partially excluding the door from the frame. 

% In PyBullet, most errors arise when the VLM fails to predict the necessary 'locate object' action before attempting to pick or place it, or when it does not fully resolve the initial instruction. The model may generate a new plan that corrects an observed dropping error but fails to account for objects already placed in their correct positions. As a result, the agent incorrectly relocates blocks that were previously positioned correctly. This behavior was particularly noticeable in the outputs of smaller models, whereas GPT and Gemini demonstrated a better understanding of why already placed blocks should not be moved.
% Additionally, despite all random parameters being fixed, some intrinsic environment stochasticity was observed. In certain cases, blocks that were randomly dropped fell into positions where the gripper was unable to pick them up, leading to further task failures.

\section{CONCLUSION}
\label{sec:conclusion}
Large Language Models excel in robotic task planning but struggle with dynamic changes and execution failures due to their reliance on textual input. To address this, we introduce LERa -- a Visual Language Model-based replanning approach that leverages visual feedback for effective replanning. Unlike existing methods, LERa requires minimal additional information, making it a more practical and adaptable solution across different agent architectures.
We also propose three new environments, including ALFRED-ChaOS and VirtualHome-ChaOS, to evaluate replanning in dynamic settings. Extensive experiments show that LERa outperforms baselines in handling environmental and execution errors. Additionally, we integrate LERa into a tabletop manipulation robot, validating its effectiveness in real-world scenarios. Our findings highlight the robustness and generalizability of LERa. In future work, we will extend its capabilities to handle a broader range of errors, especially in long-horizon tasks requiring complex temporal and spatial reasoning. We believe that our work not only advances VLM-based replanning but also provides valuable benchmarks for future research in robotic task execution.

\bibliographystyle{IEEEtran}
\bibliography{IEEEexample}
\end{document}