{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALFRED language model dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ALFRED_task_helper as alf # for dataset generation in FILM manner\n",
    "from alfred_utils.gen.constants import * # ALFRED constants\n",
    "\n",
    "exclude = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset generation\n",
    "\n",
    "The \"make_dataset\" function processes ALFRED traj_data of the chosen split (train, valid_seen, valid_unseen) and outputs the NL instructions of the queried type (film, recept, no_recept) with the corresponding lists of subtasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_SCENE_NAMES = 'alfred_utils/data/splits/'\n",
    "PATH_TO_JSON = 'alfred_utils/data/'\n",
    "\n",
    "def make_dataset(split: str, instr_type: str):\n",
    "   scene_names = json.load(open(PATH_TO_SCENE_NAMES + 'oct21.json'))\n",
    "   frames = []\n",
    "   split_data = scene_names[split]\n",
    "   for i, e in enumerate(split_data):\n",
    "        frame = {}\n",
    "        r_idx = e['repeat_idx']\n",
    "        task = e['task']\n",
    "        path_to_json = PATH_TO_JSON + 'json_2.1.0/'+ f'/{task}/pp/ann_{r_idx}.json'\n",
    "        traj_data = json.load(open(path_to_json, 'r'))\n",
    "\n",
    "        # Extract NL goal and step-by-step high-level instructions\n",
    "        anns = traj_data['turk_annotations']['anns']\n",
    "        goal = anns[r_idx]['task_desc'].lower().strip().replace('\\n', '')\n",
    "        goal = ''.join(ch for ch in goal if ch not in exclude)\n",
    "        high_descs = [\n",
    "            ''.join(\n",
    "                ch for ch in desc if ch not in exclude\n",
    "                ).lower().strip().replace('\\n', '') \n",
    "                for desc in anns[r_idx]['high_descs']]\n",
    "\n",
    "        # Get list of subtasks with the required instructions type\n",
    "        if instr_type == 'film':\n",
    "            # FILM instructions processing procedures \n",
    "            list_of_actions = alf.get_list_of_highlevel_actions(traj_data)[0]\n",
    "        elif instr_type in ('no_recept', 'recept'):\n",
    "            list_of_actions = get_actions(traj_data['plan'], high_descs, instr_type)\n",
    "        else:\n",
    "            print('Unknown instructions type')\n",
    "            return\n",
    "\n",
    "        frame['nl'] = goal + ' . ' + ' . '.join(high_descs)\n",
    "        frame['code'] = ' ; '.join([' '.join(t) for t in list_of_actions]).strip()\n",
    "        frame['r_idx'] = r_idx\n",
    "        frame['task_id'] = traj_data['task_id']\n",
    "        frame['list_of_actions'] = list_of_actions\n",
    "        frames.append(frame)\n",
    "        \n",
    "   return frames\n",
    "\n",
    "\n",
    "def get_actions(plan, high_descs, instr_type: str):\n",
    "    \"\"\"\n",
    "    Custom function for ground-truth trajectories generation. \n",
    "    For 'no_recept' and 'recept' instruction types.\n",
    "    \"\"\"\n",
    "    list_of_actions = []\n",
    "    low_actions = plan['low_actions']\n",
    "    high_actions = plan['high_pddl']\n",
    "    for i, _ in enumerate(high_descs):\n",
    "        step_actions = [act for act in low_actions if act['high_idx'] == i]\n",
    "        for act in step_actions:\n",
    "            task = act['discrete_action']['action']\n",
    "            # Select action that involves object interacting\n",
    "            if 'objectId' in act['api_action'].keys():\n",
    "                tokens = act['api_action']['objectId'].split('|')\n",
    "                obj = tokens[0]\n",
    "                if obj in VAL_ACTION_OBJECTS['Sliceable'] and 'Sliced' in tokens[-1]:\n",
    "                    obj = tokens[-1][:-2]\n",
    "                recept = None\n",
    "                # Search for receptacle in traj_data\n",
    "                if 'receptacleObjectId' in act['api_action'].keys():\n",
    "                    recept = act['api_action']['receptacleObjectId'].split('|')[0]\n",
    "                    # Append Basin to some class names\n",
    "                    if recept == 'Sink':\n",
    "                        recept = 'SinkBasin'\n",
    "                    if recept == 'Bathtub':\n",
    "                        recept = 'BathtubBasin'\n",
    "                \n",
    "                # For some objects the receptacle can be found in\n",
    "                # 'coordinateReceptacleObjectId' field\n",
    "                elif (\n",
    "                    'coordinateReceptacleObjectId' \n",
    "                    in high_actions[i]['planner_action'].keys() \n",
    "                    and obj in set(NON_RECEPTACLES) | set(MOVABLE_RECEPTACLES)):\n",
    "                     recept = high_actions[i]['planner_action']['coordinateReceptacleObjectId'][0]\n",
    "                \n",
    "                else:\n",
    "                    recept = 'None'\n",
    "                \n",
    "                # Subtasks for 'recept' are triplets, for 'no_recept' -- pairs\n",
    "                if instr_type == 'recept':            \n",
    "                    list_of_actions.append((obj, recept, task))\n",
    "                \n",
    "                elif instr_type == 'no_recept':\n",
    "                    if task != 'PutObject':\n",
    "                        list_of_actions.append((obj, task))\n",
    "                    else:\n",
    "                        list_of_actions.append((recept, task))\n",
    "                \n",
    "                    \n",
    "\n",
    "\n",
    "    return list_of_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'valid_seen' # can also take the value of 'train', 'valid_unseen'\n",
    "instr_type = 'no_recept' # can also take the value of 'film', 'recept'\n",
    "frames = make_dataset(split, instr_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ButterKnife', 'PickupObject'),\n",
       " ('Fridge', 'OpenObject'),\n",
       " ('Potato', 'SliceObject'),\n",
       " ('Fridge', 'CloseObject'),\n",
       " ('Microwave', 'OpenObject'),\n",
       " ('Microwave', 'PutObject'),\n",
       " ('Microwave', 'CloseObject'),\n",
       " ('Fridge', 'OpenObject'),\n",
       " ('PotatoSliced', 'PickupObject'),\n",
       " ('Fridge', 'CloseObject'),\n",
       " ('Microwave', 'OpenObject'),\n",
       " ('Microwave', 'PutObject'),\n",
       " ('Microwave', 'CloseObject'),\n",
       " ('Microwave', 'ToggleObjectOn'),\n",
       " ('Microwave', 'ToggleObjectOff'),\n",
       " ('Microwave', 'OpenObject'),\n",
       " ('PotatoSliced', 'PickupObject'),\n",
       " ('Microwave', 'CloseObject'),\n",
       " ('SinkBasin', 'PutObject')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames[0]['list_of_actions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training sets for [CodeT5 training](https://github.com/salesforce/codet5)\n",
    "\n",
    "The function below is used to create json files for CodeT5 training with special names (train.json, dev.json, test.json)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'fiqa/language_processing/processed_instructions/'\n",
    "\n",
    "def make_codet5_training_set(frames: dict, split: str):\n",
    "    \n",
    "    # Shuffle the tasks\n",
    "    shuffler = np.random.permutation(len(frames))\n",
    "    frames = np.array(frames)[shuffler]\n",
    "\n",
    "    # Create a file with the proper name for CodeT5 training,\n",
    "    # validation and testing \n",
    "    if split == 'train':\n",
    "        file_name = 'train'\n",
    "    elif split == 'valid_seen':\n",
    "        file_name = 'dev'\n",
    "    elif split == 'valid_unseen':\n",
    "        file_name = 'test'\n",
    "    with open(OUTPUT_PATH + f'{file_name}.json', 'w') as f:\n",
    "        # We need only natural language text and the sequence of subtasks (code)\n",
    "        for frame in frames:\n",
    "            new_frame = {}\n",
    "            new_frame['code'] = frame['code']\n",
    "            new_frame['nl'] = frame['nl']\n",
    "            json_data = json.dumps(new_frame)\n",
    "            f.write(json_data + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this multiple times with different splits to create train, dev and test sets for CodeT5 training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_codet5_training_set(frames, split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create files with GT trajectories for FIQA oracle agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gt_trajectories(frames: dict, split: str, instr_type: str):\n",
    "    new_frames = {}\n",
    "    for frame in frames:\n",
    "        task_key = (frame['task_id'], frame['r_idx'])\n",
    "        new_frames[task_key] = frame['list_of_actions']\n",
    "    with open(\n",
    "       OUTPUT_PATH + f'{split}_{instr_type}_gt_alfred.p', 'wb') as f:\n",
    "     pickle.dump(new_frames, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this multiple times with different splits to create GT trajectories for train, valid_seen and unseen. These files has to be processed by the lp_outputs.py script inside FIQA to obtain the GT instructions with the navigation inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_gt_trajectories(frames, split, instr_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
